"""
Agent Wrapper com InteligÃªncia Ativa
Integra ferramentas, avaliaÃ§Ã£o de qualidade e comportamento inteligente
"""

import asyncio
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass
from datetime import datetime
import logging
import json

from app.core.llm_router import intelligent_llm_call
from app.core.quality_evaluator import QualityEvaluator, GoldenDataset, EvaluationResult
from app.orch.tools import ToolRegistry
from app.orch.registry_init import build_tool_registry

logger = logging.getLogger(__name__)

@dataclass
class AgentExecution:
    """Resultado da execuÃ§Ã£o de um agente"""
    agent_id: str
    prompt: str
    response: str
    tools_used: List[str]
    execution_time: float
    timestamp: datetime
    evaluation_result: Optional[EvaluationResult] = None
    error: Optional[str] = None

class IntelligentAgent:
    """
    Agente inteligente que:
    1. Usa ferramentas ativamente para buscar informaÃ§Ãµes
    2. Se auto-avalia continuamente
    3. Aprende com feedback e melhora performance
    """
    
    def __init__(
        self,
        agent_id: str,
        tenant_id: str,
        system_message: str,
        model_provider: str = "openai",
        model_name: str = "gpt-4",
        tools_config: Optional[Dict[str, Any]] = None,
        enable_evaluation: bool = True
    ):
        self.agent_id = agent_id
        self.tenant_id = tenant_id
        self.system_message = system_message
        self.model_provider = model_provider
        self.model_name = model_name
        self.enable_evaluation = enable_evaluation
        
        # Inicializa registry de ferramentas
        self.tool_registry = build_tool_registry(tenant_id, tools_config or {})
        
        # Sistema de avaliaÃ§Ã£o
        if enable_evaluation:
            self.golden_dataset = GoldenDataset()
            self.evaluator = QualityEvaluator(self.golden_dataset)
        
        # HistÃ³rico de execuÃ§Ãµes
        self.execution_history: List[AgentExecution] = []
        
        # MÃ©tricas de performance
        self.performance_metrics = {
            "total_executions": 0,
            "successful_executions": 0,
            "avg_execution_time": 0.0,
            "avg_quality_score": 0.0,
            "tools_usage_count": {},
            "error_count": 0
        }
    
    async def execute(self, prompt: str, evaluate_quality: Optional[bool] = None) -> AgentExecution:
        """
        Executa o agente com inteligÃªncia ativa
        
        Args:
            prompt: Prompt do usuÃ¡rio
            evaluate_quality: Se deve avaliar qualidade (padrÃ£o: usa configuraÃ§Ã£o do agente)
        
        Returns:
            Resultado da execuÃ§Ã£o com mÃ©tricas
        """
        
        start_time = asyncio.get_event_loop().time()
        evaluate_quality = evaluate_quality if evaluate_quality is not None else self.enable_evaluation
        
        logger.info(f"Iniciando execuÃ§Ã£o do agente {self.agent_id}")
        logger.info(f"Prompt: {prompt[:100]}...")
        
        execution = AgentExecution(
            agent_id=self.agent_id,
            prompt=prompt,
            response="",
            tools_used=[],
            execution_time=0.0,
            timestamp=datetime.now()
        )
        
        try:
            # Executa o LLM com ferramentas
            response = await intelligent_llm_call(
                prompt=prompt,
                system_message=self.system_message,
                tool_registry=self.tool_registry,
                model_provider=self.model_provider,
                model_name=self.model_name
            )
            
            execution.response = response
            execution.execution_time = asyncio.get_event_loop().time() - start_time
            
            # Captura ferramentas utilizadas (simulado - na produÃ§Ã£o seria capturado do LLM router)
            execution.tools_used = self._extract_tools_used(response)
            
            # AvaliaÃ§Ã£o de qualidade
            if evaluate_quality:
                execution.evaluation_result = await self._evaluate_execution(execution)
            
            # Atualiza mÃ©tricas
            self._update_metrics(execution)
            
            logger.info(f"ExecuÃ§Ã£o concluÃ­da com sucesso em {execution.execution_time:.2f}s")
            
        except Exception as e:
            logger.error(f"Erro na execuÃ§Ã£o do agente {self.agent_id}: {e}")
            execution.error = str(e)
            execution.execution_time = asyncio.get_event_loop().time() - start_time
            self.performance_metrics["error_count"] += 1
        
        # Adiciona ao histÃ³rico
        self.execution_history.append(execution)
        
        return execution
    
    def _extract_tools_used(self, response: str) -> List[str]:
        """Extrai ferramentas utilizadas da resposta (simulado)"""
        # Na produÃ§Ã£o, isso seria capturado do LLM router
        tools = []
        
        if "jurisprudÃªncia" in response.lower():
            tools.append("buscar_jurisprudencia_recente")
        
        if "documento interno" in response.lower():
            tools.append("buscar_documento_interno")
        
        if "valor" in response.lower() and "causa" in response.lower():
            tools.append("calcular_valor_causa")
        
        if "validar" in response.lower() and "fundamentaÃ§Ã£o" in response.lower():
            tools.append("validar_fundamentacao_legal")
        
        if "legislaÃ§Ã£o" in response.lower() or "lei" in response.lower():
            tools.append("buscar_legislacao_atualizada")
        
        return tools
    
    async def _evaluate_execution(self, execution: AgentExecution) -> Optional[EvaluationResult]:
        """Avalia a qualidade da execuÃ§Ã£o"""
        
        if not self.enable_evaluation:
            return None
        
        try:
            # Busca amostra similar no golden dataset
            similar_sample = self._find_similar_sample(execution.prompt)
            
            if similar_sample:
                # Cria funÃ§Ã£o de agente para avaliaÃ§Ã£o
                async def agent_function(prompt: str) -> str:
                    return execution.response
                
                # Avalia usando o golden dataset
                evaluation_result = await self.evaluator.evaluate_sample(
                    similar_sample, 
                    agent_function, 
                    execution.tools_used
                )
                
                logger.info(f"AvaliaÃ§Ã£o concluÃ­da para execuÃ§Ã£o {execution.agent_id}")
                return evaluation_result
            
        except Exception as e:
            logger.error(f"Erro na avaliaÃ§Ã£o: {e}")
        
        return None
    
    def _find_similar_sample(self, prompt: str) -> Optional[Any]:
        """Encontra amostra similar no golden dataset"""
        # ImplementaÃ§Ã£o simples baseada em palavras-chave
        prompt_lower = prompt.lower()
        
        for sample in self.golden_dataset.samples:
            sample_words = set(sample.input_prompt.lower().split())
            prompt_words = set(prompt_lower.split())
            
            # Calcula similaridade
            intersection = sample_words.intersection(prompt_words)
            union = sample_words.union(prompt_words)
            
            if union and len(intersection) / len(union) > 0.3:  # 30% similaridade
                return sample
        
        return None
    
    def _update_metrics(self, execution: AgentExecution):
        """Atualiza mÃ©tricas de performance"""
        
        self.performance_metrics["total_executions"] += 1
        
        if execution.error is None:
            self.performance_metrics["successful_executions"] += 1
        
        # Atualiza tempo mÃ©dio
        total_time = (self.performance_metrics["avg_execution_time"] * 
                     (self.performance_metrics["total_executions"] - 1) + 
                     execution.execution_time)
        self.performance_metrics["avg_execution_time"] = total_time / self.performance_metrics["total_executions"]
        
        # Atualiza uso de ferramentas
        for tool in execution.tools_used:
            self.performance_metrics["tools_usage_count"][tool] = (
                self.performance_metrics["tools_usage_count"].get(tool, 0) + 1
            )
        
        # Atualiza score de qualidade
        if execution.evaluation_result:
            current_score = self.performance_metrics["avg_quality_score"]
            new_score = sum(execution.evaluation_result.metrics.values()) / len(execution.evaluation_result.metrics)
            
            total_evaluations = len([e for e in self.execution_history if e.evaluation_result])
            
            if total_evaluations > 0:
                self.performance_metrics["avg_quality_score"] = (
                    (current_score * (total_evaluations - 1) + new_score) / total_evaluations
                )
    
    def get_performance_report(self) -> str:
        """Gera relatÃ³rio de performance do agente"""
        
        metrics = self.performance_metrics
        
        report = f"""
=== RELATÃ“RIO DE PERFORMANCE - AGENTE {self.agent_id} ===

ğŸ¤– INFORMAÃ‡Ã•ES GERAIS:
â€¢ Tenant: {self.tenant_id}
â€¢ Modelo: {self.model_provider}/{self.model_name}
â€¢ AvaliaÃ§Ã£o Habilitada: {'Sim' if self.enable_evaluation else 'NÃ£o'}

ğŸ“Š MÃ‰TRICAS DE EXECUÃ‡ÃƒO:
â€¢ Total de ExecuÃ§Ãµes: {metrics['total_executions']}
â€¢ ExecuÃ§Ãµes Bem-sucedidas: {metrics['successful_executions']}
â€¢ Taxa de Sucesso: {metrics['successful_executions'] / max(metrics['total_executions'], 1):.2%}
â€¢ Tempo MÃ©dio: {metrics['avg_execution_time']:.2f}s
â€¢ Erros: {metrics['error_count']}

ğŸ“ˆ QUALIDADE:
â€¢ Score MÃ©dio de Qualidade: {metrics['avg_quality_score']:.2%}

ğŸ”§ USO DE FERRAMENTAS:
"""
        
        for tool, count in metrics['tools_usage_count'].items():
            report += f"â€¢ {tool}: {count} usos\n"
        
        if not metrics['tools_usage_count']:
            report += "â€¢ Nenhuma ferramenta utilizada\n"
        
        # Ãšltimas execuÃ§Ãµes
        recent_executions = self.execution_history[-5:]  # Ãšltimas 5
        
        report += f"\nğŸ• ÃšLTIMAS EXECUÃ‡Ã•ES:\n"
        for exec in recent_executions:
            status = "âœ…" if exec.error is None else "âŒ"
            quality = ""
            if exec.evaluation_result:
                avg_score = sum(exec.evaluation_result.metrics.values()) / len(exec.evaluation_result.metrics)
                quality = f" (Q: {avg_score:.1%})"
            
            report += f"â€¢ {exec.timestamp.strftime('%H:%M:%S')} {status} {exec.execution_time:.2f}s{quality}\n"
        
        return report
    
    async def auto_evaluate(self, sample_count: int = 3) -> Dict[str, Any]:
        """
        Auto-avaliaÃ§Ã£o usando amostras do golden dataset
        
        Args:
            sample_count: NÃºmero de amostras para testar
            
        Returns:
            Resultados da avaliaÃ§Ã£o
        """
        
        if not self.enable_evaluation:
            return {"error": "AvaliaÃ§Ã£o nÃ£o habilitada para este agente"}
        
        # Seleciona amostras aleatÃ³rias
        import random
        samples = random.sample(self.golden_dataset.samples, min(sample_count, len(self.golden_dataset.samples)))
        
        results = []
        
        for sample in samples:
            logger.info(f"Auto-avaliaÃ§Ã£o com amostra {sample.id}")
            
            # Executa o agente
            execution = await self.execute(sample.input_prompt, evaluate_quality=True)
            results.append(execution)
        
        # Calcula mÃ©tricas agregadas
        evaluation_results = [r.evaluation_result for r in results if r.evaluation_result]
        
        if not evaluation_results:
            return {"error": "Nenhuma avaliaÃ§Ã£o foi possÃ­vel"}
        
        # MÃ©tricas agregadas
        total_score = sum(
            sum(result.metrics.values()) / len(result.metrics) 
            for result in evaluation_results
        )
        
        avg_score = total_score / len(evaluation_results)
        
        return {
            "avg_quality_score": avg_score,
            "samples_tested": len(samples),
            "successful_evaluations": len(evaluation_results),
            "results": results,
            "detailed_metrics": {
                metric.value: sum(result.metrics[metric] for result in evaluation_results) / len(evaluation_results)
                for metric in evaluation_results[0].metrics.keys()
            }
        }

# Sistema de gerenciamento de agentes
class AgentManager:
    """Gerencia mÃºltiplos agentes inteligentes"""
    
    def __init__(self):
        self.agents: Dict[str, IntelligentAgent] = {}
    
    def create_agent(
        self, 
        agent_id: str, 
        tenant_id: str, 
        system_message: str,
        **kwargs
    ) -> IntelligentAgent:
        """Cria um novo agente"""
        
        agent = IntelligentAgent(
            agent_id=agent_id,
            tenant_id=tenant_id,
            system_message=system_message,
            **kwargs
        )
        
        self.agents[agent_id] = agent
        logger.info(f"Agente {agent_id} criado para tenant {tenant_id}")
        
        return agent
    
    def get_agent(self, agent_id: str) -> Optional[IntelligentAgent]:
        """Busca um agente especÃ­fico"""
        return self.agents.get(agent_id)
    
    async def execute_agent(self, agent_id: str, prompt: str) -> Optional[AgentExecution]:
        """Executa um agente especÃ­fico"""
        agent = self.get_agent(agent_id)
        
        if not agent:
            logger.error(f"Agente {agent_id} nÃ£o encontrado")
            return None
        
        return await agent.execute(prompt)
    
    def get_all_agents_report(self) -> str:
        """Gera relatÃ³rio de todos os agentes"""
        
        if not self.agents:
            return "Nenhum agente registrado."
        
        report = "=== RELATÃ“RIO GERAL DE AGENTES ===\n\n"
        
        for agent_id, agent in self.agents.items():
            report += f"ğŸ“‹ {agent_id}:\n"
            report += f"â€¢ Tenant: {agent.tenant_id}\n"
            report += f"â€¢ ExecuÃ§Ãµes: {agent.performance_metrics['total_executions']}\n"
            report += f"â€¢ Taxa de Sucesso: {agent.performance_metrics['successful_executions'] / max(agent.performance_metrics['total_executions'], 1):.2%}\n"
            report += f"â€¢ Qualidade MÃ©dia: {agent.performance_metrics['avg_quality_score']:.2%}\n\n"
        
        return report

# Exemplo de uso
async def exemplo_agent_inteligente():
    """Demonstra o uso do agente inteligente"""
    
    # Cria gerenciador
    manager = AgentManager()
    
    # Cria agente jurÃ­dico
    agent = manager.create_agent(
        agent_id="juridico_001",
        tenant_id="cliente_abc",
        system_message="VocÃª Ã© um assistente jurÃ­dico especializado em direito civil e administrativo. Use as ferramentas disponÃ­veis para buscar informaÃ§Ãµes precisas e atualizadas.",
        model_provider="openai",
        model_name="gpt-4",
        enable_evaluation=True
    )
    
    # Executa algumas tarefas
    prompts = [
        "Preciso de jurisprudÃªncia recente do STJ sobre rescisÃ£o de contratos administrativos",
        "Qual o valor da causa para uma aÃ§Ã£o de indenizaÃ§Ã£o de R$ 25.000?",
        "Valide esta fundamentaÃ§Ã£o: O contrato deve ser rescindido com base no art. 78 da Lei 8.666/93"
    ]
    
    for prompt in prompts:
        print(f"\nğŸ¤– Executando: {prompt}")
        execution = await agent.execute(prompt)
        print(f"âœ… Resposta: {execution.response[:100]}...")
        print(f"â±ï¸  Tempo: {execution.execution_time:.2f}s")
        print(f"ğŸ”§ Ferramentas: {execution.tools_used}")
    
    # RelatÃ³rio de performance
    print("\n" + agent.get_performance_report())
    
    # Auto-avaliaÃ§Ã£o
    print("\nğŸ” Iniciando auto-avaliaÃ§Ã£o...")
    eval_results = await agent.auto_evaluate(sample_count=3)
    print(f"ğŸ“Š Score de qualidade: {eval_results.get('avg_quality_score', 0):.2%}")

if __name__ == "__main__":
    asyncio.run(exemplo_agent_inteligente())
