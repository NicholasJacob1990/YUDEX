"""
Teste e demonstraÃ§Ã£o da Onda 2: InteligÃªncia Ativa e Qualidade MensurÃ¡vel
"""

import asyncio
import logging
from app.core.intelligent_agent import AgentManager, IntelligentAgent
from app.core.quality_evaluator import GoldenDataset, QualityEvaluator
from app.orch.registry_init import build_tool_registry

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def demonstracao_onda2():
    """DemonstraÃ§Ã£o completa da Onda 2"""
    
    print("=" * 60)
    print("ğŸŒŠ ONDA 2: INTELIGÃŠNCIA ATIVA E QUALIDADE MENSURÃVEL")
    print("=" * 60)
    
    # 1. CriaÃ§Ã£o do sistema de agentes inteligentes
    print("\nğŸ“‹ 1. CRIANDO SISTEMA DE AGENTES INTELIGENTES")
    print("-" * 40)
    
    manager = AgentManager()
    
    # Agente especializado em direito civil
    agente_civil = manager.create_agent(
        agent_id="civil_specialist",
        tenant_id="escritorio_abc",
        system_message="""VocÃª Ã© um especialista em direito civil brasileiro. 
        Use as ferramentas disponÃ­veis para buscar jurisprudÃªncia, legislaÃ§Ã£o e 
        documentos internos. Sempre fundamente suas respostas em fontes confiÃ¡veis.""",
        model_provider="openai",
        model_name="gpt-4",
        enable_evaluation=True
    )
    
    # Agente especializado em direito administrativo
    agente_admin = manager.create_agent(
        agent_id="admin_specialist",
        tenant_id="escritorio_abc",
        system_message="""VocÃª Ã© um especialista em direito administrativo brasileiro.
        Foque em licitaÃ§Ãµes, contratos administrativos e procedimentos da administraÃ§Ã£o pÃºblica.
        Use as ferramentas para buscar informaÃ§Ãµes atualizadas.""",
        model_provider="anthropic",
        model_name="claude-3-sonnet",
        enable_evaluation=True
    )
    
    print(f"âœ… Agente Civil criado: {agente_civil.agent_id}")
    print(f"âœ… Agente Administrativo criado: {agente_admin.agent_id}")
    
    # 2. DemonstraÃ§Ã£o de execuÃ§Ã£o com ferramentas
    print("\nğŸ”§ 2. DEMONSTRANDO USO ATIVO DE FERRAMENTAS")
    print("-" * 40)
    
    casos_teste = [
        {
            "agente": agente_civil,
            "prompt": "Preciso de jurisprudÃªncia recente sobre responsabilidade civil por danos morais em contratos de prestaÃ§Ã£o de serviÃ§os",
            "descricao": "Busca de jurisprudÃªncia civil"
        },
        {
            "agente": agente_admin,
            "prompt": "Qual o valor da causa para uma aÃ§Ã£o questionando licitaÃ§Ã£o no valor de R$ 500.000? Consulte tambÃ©m a legislaÃ§Ã£o aplicÃ¡vel",
            "descricao": "CÃ¡lculo de valor + consulta legislaÃ§Ã£o"
        },
        {
            "agente": agente_civil,
            "prompt": "Valide a fundamentaÃ§Ã£o legal: 'O contrato deve ser rescindido com base no art. 475 do CÃ³digo Civil por onerosidade excessiva'",
            "descricao": "ValidaÃ§Ã£o de fundamentaÃ§Ã£o legal"
        }
    ]
    
    execucoes = []
    
    for caso in casos_teste:
        print(f"\nğŸ¤– Executando: {caso['descricao']}")
        print(f"ğŸ“ Prompt: {caso['prompt']}")
        
        execution = await caso['agente'].execute(caso['prompt'])
        execucoes.append(execution)
        
        print(f"âœ… Agente: {execution.agent_id}")
        print(f"â±ï¸  Tempo: {execution.execution_time:.2f}s")
        print(f"ğŸ”§ Ferramentas utilizadas: {', '.join(execution.tools_used) if execution.tools_used else 'Nenhuma'}")
        print(f"ğŸ“„ Resposta: {execution.response[:150]}...")
        
        if execution.evaluation_result:
            avg_score = sum(execution.evaluation_result.metrics.values()) / len(execution.evaluation_result.metrics)
            print(f"ğŸ“Š Score de qualidade: {avg_score:.2%}")
    
    # 3. DemonstraÃ§Ã£o do sistema de avaliaÃ§Ã£o
    print("\nğŸ“Š 3. SISTEMA DE AVALIAÃ‡ÃƒO DE QUALIDADE")
    print("-" * 40)
    
    # Cria dataset de golden samples
    golden_dataset = GoldenDataset()
    print(f"ğŸ“š Dataset criado com {len(golden_dataset.samples)} amostras de referÃªncia")
    
    # Lista categorias disponÃ­veis
    categorias = set(sample.category for sample in golden_dataset.samples)
    print(f"ğŸ“‚ Categorias disponÃ­veis: {', '.join(categorias)}")
    
    # Demonstra avaliaÃ§Ã£o automÃ¡tica
    print("\nğŸ” Executando avaliaÃ§Ã£o automÃ¡tica...")
    
    evaluator = QualityEvaluator(golden_dataset)
    
    # FunÃ§Ã£o de agente simulada para avaliaÃ§Ã£o
    async def agente_para_avaliacao(prompt: str) -> str:
        # Simula execuÃ§Ã£o do agente civil
        execution = await agente_civil.execute(prompt, evaluate_quality=False)
        return execution.response
    
    # Avalia em algumas amostras
    sample_ids = ["juris_001", "calc_001", "valid_001"]
    results = await evaluator.evaluate_agent(agente_para_avaliacao, sample_ids)
    
    # Gera relatÃ³rio
    report = evaluator.generate_report(results)
    print(report)
    
    # 4. Auto-avaliaÃ§Ã£o dos agentes
    print("\nğŸ”„ 4. AUTO-AVALIAÃ‡ÃƒO DOS AGENTES")
    print("-" * 40)
    
    for agente in [agente_civil, agente_admin]:
        print(f"\nğŸ¤– Auto-avaliando agente: {agente.agent_id}")
        
        eval_results = await agente.auto_evaluate(sample_count=2)
        
        if "error" not in eval_results:
            print(f"ğŸ“Š Score mÃ©dio: {eval_results['avg_quality_score']:.2%}")
            print(f"ğŸ§ª Amostras testadas: {eval_results['samples_tested']}")
            print(f"âœ… AvaliaÃ§Ãµes bem-sucedidas: {eval_results['successful_evaluations']}")
            
            # MÃ©tricas detalhadas
            print("ğŸ“ˆ MÃ©tricas detalhadas:")
            for metric, value in eval_results['detailed_metrics'].items():
                print(f"  â€¢ {metric.replace('_', ' ').title()}: {value:.2%}")
        else:
            print(f"âŒ {eval_results['error']}")
    
    # 5. RelatÃ³rio de performance
    print("\nğŸ“ˆ 5. RELATÃ“RIO DE PERFORMANCE")
    print("-" * 40)
    
    for agente in [agente_civil, agente_admin]:
        report = agente.get_performance_report()
        print(report)
    
    # 6. RelatÃ³rio geral do sistema
    print("\nğŸ“‹ 6. RELATÃ“RIO GERAL DO SISTEMA")
    print("-" * 40)
    
    general_report = manager.get_all_agents_report()
    print(general_report)
    
    # 7. DemonstraÃ§Ã£o de melhoria contÃ­nua
    print("\nğŸ”„ 7. DEMONSTRAÃ‡ÃƒO DE MELHORIA CONTÃNUA")
    print("-" * 40)
    
    print("ğŸ¯ Executando mais casos para demonstrar aprendizado...")
    
    # Executa mais casos no agente civil
    casos_adicionais = [
        "Busque jurisprudÃªncia sobre contratos de compra e venda com vÃ­cio do produto",
        "Calcule o valor da causa para aÃ§Ã£o de cobranÃ§a de R$ 15.000",
        "Valide: 'A garantia deve ser executada conforme art. 441 do CC'"
    ]
    
    for prompt in casos_adicionais:
        await agente_civil.execute(prompt)
    
    # Mostra evoluÃ§Ã£o das mÃ©tricas
    print(f"\nğŸ“Š MÃ©tricas apÃ³s execuÃ§Ãµes adicionais:")
    print(f"â€¢ Total de execuÃ§Ãµes: {agente_civil.performance_metrics['total_executions']}")
    print(f"â€¢ Taxa de sucesso: {agente_civil.performance_metrics['successful_executions'] / agente_civil.performance_metrics['total_executions']:.2%}")
    print(f"â€¢ Tempo mÃ©dio: {agente_civil.performance_metrics['avg_execution_time']:.2f}s")
    
    # 8. ConclusÃ£o
    print("\nğŸ‰ 8. CONCLUSÃƒO - ONDA 2 IMPLEMENTADA")
    print("-" * 40)
    
    print("âœ… Funcionalidades implementadas:")
    print("  â€¢ Agentes com uso ativo de ferramentas")
    print("  â€¢ Sistema de avaliaÃ§Ã£o de qualidade automatizada")
    print("  â€¢ Golden dataset para benchmarking")
    print("  â€¢ MÃ©tricas de performance em tempo real")
    print("  â€¢ Auto-avaliaÃ§Ã£o e melhoria contÃ­nua")
    print("  â€¢ RelatÃ³rios detalhados de qualidade")
    print("  â€¢ Suporte a mÃºltiplos LLMs (OpenAI, Anthropic)")
    print("  â€¢ Ferramentas especializadas para domÃ­nio jurÃ­dico")
    
    print("\nğŸš€ O sistema Harvey agora possui inteligÃªncia ativa e qualidade mensurÃ¡vel!")
    print("=" * 60)

async def teste_rapido():
    """Teste rÃ¡pido das funcionalidades principais"""
    
    print("ğŸ§ª TESTE RÃPIDO - ONDA 2")
    print("-" * 30)
    
    # Cria agente simples
    manager = AgentManager()
    agente = manager.create_agent(
        agent_id="teste_001",
        tenant_id="teste",
        system_message="VocÃª Ã© um assistente jurÃ­dico de teste.",
        enable_evaluation=True
    )
    
    # Executa caso simples
    execution = await agente.execute("Preciso de jurisprudÃªncia sobre contratos administrativos")
    
    print(f"âœ… ExecuÃ§Ã£o concluÃ­da:")
    print(f"  â€¢ Tempo: {execution.execution_time:.2f}s")
    print(f"  â€¢ Ferramentas: {execution.tools_used}")
    print(f"  â€¢ Resposta: {execution.response[:100]}...")
    
    # Mostra mÃ©tricas
    metrics = agente.performance_metrics
    print(f"ğŸ“Š MÃ©tricas: {metrics['total_executions']} execuÃ§Ãµes, {metrics['successful_executions']} sucessos")

if __name__ == "__main__":
    # Executa demonstraÃ§Ã£o completa
    asyncio.run(demonstracao_onda2())
    
    # Ou executa teste rÃ¡pido
    # asyncio.run(teste_rapido())
